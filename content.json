{"meta":{"title":"Moonm3n's blog","subtitle":"","description":"","author":"Yueyang Zhan","url":"https://moonm3n.github.io","root":"/"},"pages":[{"title":"所有分类","date":"2022-04-16T14:29:58.972Z","updated":"2022-04-16T14:29:58.972Z","comments":true,"path":"categories/index.html","permalink":"https://moonm3n.github.io/categories/index.html","excerpt":"","text":""},{"title":"所有标签","date":"2022-04-16T14:31:36.491Z","updated":"2022-04-16T14:31:36.491Z","comments":true,"path":"tags/index.html","permalink":"https://moonm3n.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"TiDB-PD","slug":"TiKV-PD","date":"2022-03-22T06:45:27.000Z","updated":"2022-04-16T05:13:58.517Z","comments":true,"path":"2022/03/22/TiKV-PD/","link":"","permalink":"https://moonm3n.github.io/2022/03/22/TiKV-PD/","excerpt":"PD 是 TiDB 里的全局中心总控节点, 主要负责全局元信息的存储以及 TiKV 集群负载均衡调度.","text":"PD 是 TiDB 里的全局中心总控节点, 主要负责全局元信息的存储以及 TiKV 集群负载均衡调度. 实现原理PD 是一个逻辑上的单点, 物理上是一个集群, 集成 etcd, 支持故障恢复, 保证了强一致性.PD 功能可以分为三类: 路由 元数据管理 调度 TiKV 中的 Region Leader 与 Store 会定期向 PD 发送 Heartbeat, Heartbeat 中包含了 Region 和 Store 的各种状态信息,PD 根据状态信息来调度 TiKV 集群的负载均衡, 将 Operator 通过 Heartbeat response 回复给 TiKV. 基本概念1. SchedulerScheduler 是用来调度资源的接口, 调度器通过状态信息生成 Operator. 2. OperatorOperator 是 PD 对 TiKV 的调度操作的集合, 可以由其他 Operator 组合而成. Selector/FilterSelector 与 Filter 负责选择调度操作的 source 与 target. ControllerController 负责控制整个调度的速度. CoordinatorCoordinator 在 Region Heartbeat 会检测 Region 是否需要调度, 如果需要, 则进行调度. PD 中有许多调度器, 每个调度器是独立运行的, 有着不同的调度目的.常见的调度器有: balance-leader-scheduler: 保持不同节点的 Leader 均衡. balance-region-scheduler: 保持不同节点的 Region 均衡. hot-region-scheduler: 保持不同节点的读写热点 Region 均衡. evict-leader-{store-id}: 驱逐某个节点的所有 Leader. 调度流程调度的流程大体上可以分为三部分: 信息收集Region Leader 周期性地上报 RegionHeartbeat 心跳, 包含了 Region 范围, 副本分布, 副本状态, 数据量, 读写流量等数据.Store 周期性地上报 StoreHeartbeat 心跳, 包含了 Store 的基本信息, 容量, 剩余空间, 读写流量等数据. 生成调度 执行调度将 Operator Step 下发给对应 Region 的 Leader. 集群的元信息、TSO 信息、Region 信息 持久化在 etcd 中.Store 与 Region 的状态存在 cache 中.","categories":[{"name":"databases","slug":"databases","permalink":"https://moonm3n.github.io/categories/databases/"}],"tags":[{"name":"TiDB","slug":"TiDB","permalink":"https://moonm3n.github.io/tags/TiDB/"}]},{"title":"TinyKV Project3 MultiRaftKV PartB","slug":"tinykv-project3-MultiRaftKV-PartB","date":"2022-03-05T12:52:50.000Z","updated":"2022-04-18T04:38:16.697Z","comments":true,"path":"2022/03/05/tinykv-project3-MultiRaftKV-PartB/","link":"","permalink":"https://moonm3n.github.io/2022/03/05/tinykv-project3-MultiRaftKV-PartB/","excerpt":"TinyKV Project3 Part B.","text":"TinyKV Project3 Part B. summary3B 主要涉及到 领导人变更(transfer leader) 节点变更(conf change) 以及 region 分裂(region split). transfer leadertransfer leader 请求不需要作为一个 entry 在 Raft group 之中同步,peer 在收到该请求时, 只需要向下传递给 Raft 层执行, 完成后返回 response 即可. conf changeconf change 请求的主要作用是向 Raft group 中添加或删除 peer, 分为 add node 和 remove node 两种类型. add node: 在 Raft group 中添加节点, peer 只需要更改自己 peerStore 和 ctx.storeMeta 中的 region,向其中添加 peer 即可. store_worker 在向该 peer 发送消息时才会新建 peer. 吐槽: conf change 的消息类型以及处理方法为啥这么特别而又别扭? 是有什么特殊的考虑吗. remove node: 在 Raft group 中删除节点, 如果被删除的不是自己, peer 的行为与 add node 时类似,如果删除的是自己, 直接调用 destroyPeer() 函数即可. region split在数据写入后, split checker 会定期检测 region 的大小, 符合条件时, 生产 region split 的 key. 问题记录1. handle raft message failed storeID 2, region 1 not exists but not tombstoneprocess remove node 删除自己时, 首先调用了 destroyPeer() 修改 RaftLocalState tombstone,后续没有判断这一情况, WriteRegionState 时又将 RaftLocalState 改为了 normal. 2. panic: [region 1] 6 unexpected raft log index: lastIndex 0 &lt; appliedIndex 5164add node 在 store_worker 实际创建 peer 时报错.raftState 和 applyState 分别从 kv engine 和 raft engine 中根据 region id 读取出,applyState 是正常的, raftState 却是空的. 原因: process 多个 entry 时, conf change 调用 destroyPeer 清空了 store,下一条 entry 执行时又会更新 apply index, 重新写入了 applyState. process entry 时应该判断 d.stopped, 如果已经停止, 直接 return. 3. panic: request timeoutRaft group 中只有两个节点时, 再删除一个节点, 如果这个节点正好是 leader,可能在 commit 消息发送前便把自己 destroy 了, 另一个节点便无法得知这一情况的发生, 永远无法选举成功. 应该先 transfer leader 给剩下的节点, 直接返回错误, client 会进行重试, 再由剩下的节点来处理 remove node. 4. panic: resp.Responses[0].CmdType != raft_cmdpb.CmdType_Put在三个节点的集群中删除一个节点后会发生这个问题. 在 append proposal 处打日志后发现,同一个 index 的 proposal append 了多次. 12342022/03/17 15:32:08.334664 peer_msg_handler.go:598: \u001b[0;37m[info] [region 1] 3 append proposal type Snap, index 16681\u001b[0m2022/03/17 15:32:08.334670 peer_msg_handler.go:598: \u001b[0;37m[info] [region 1] 3 append proposal type Snap, index 16681\u001b[0m2022/03/17 15:32:08.334682 peer_msg_handler.go:598: \u001b[0;37m[info] [region 1] 3 append proposal type Put, index 16681\u001b[0m2022/03/17 15:32:08.334686 peer_msg_handler.go:598: \u001b[0;37m[info] [region 1] 3 append proposal type Put, index 16681\u001b[0m propose entry 后, Raft 的 last index 没有发生变化. transferLeader 产生的错误没有正确返回. 5. test_test.go:221: \u001b[0;31m[fatal] get wrong value, client 17与问题 4 相同. 6. panic: [region 1] 4 meta corruption detected测试用例: TestSplitConfChangeSnapshotUnreliableRecoverConcurrentPartition3B 出错代码行: /root/tinykv/kv/raftstore/peer_msg_handler.go:867 +0x417 destroyPeer() 中删除 storeMeta regionRanges 中的 regionItem 时,发现并没有对应的 regionItem. applySnapshot() 后不应该在 storeMeta 中删除 prevRegion 的 regionRanges,在分区的情况下, 一个节点并没有收到 split message,分区恢复后, 新分裂的 region 的 peer 先被创建并收到 snapshot, 向 storeMeta 中写入,另一个 peer 收到 snapshot 时如果删除 prevRegion, 就会导致 panic.prevRegin 的 endKey 恰好是另一个 peer 的已经写入的 endKey, 因此不能删除它. 7. panic: split peer count not equal to region peer count自定义的 panic, split request 中的 id 的数目可能与 目前集群中的节点数不相同, 应该拒绝这个请求. 8. panic: entries’ high 586 is out of bound, lastIndex 584leader 向 follower 连续发送两次快照, follower 应用第一个快照并返回 response 后,leader 会向 follower 发送 append entries, 此时 follower 应用了第二个快照,follower 接受到 entries 后发现他们的 index 比自己的 last index 小, 尝试替换自己的日志,但自己又没有这些日志, 便会引发这个错误. 解决方法: 不调用 panic 直接返回即可. 9. request timeout3B 中会遇到各种各样的 request timeout. 9.1 send message err: message is dropped在没有分区的情况下, leader 一个时间段内发送的所有消息都丢失了. 没有查明原因, 这个问题会导致 leader 发送的 snapshot 很容易丢失,又因为在 2C 中优化了快照的发送次数, 很容易导致超时. 9.2. tombstone peer receives a stale message超时前出现不停出现 tombstone peer receives a stale message 的日志. 9.3 1_12_14677 is registered more than 1 timesend snapshot 后出现这条错误. 超时前不停地出现这条错误.当集群中只有两个节点 1, 2 时, 1 向 2 发送快照, 2 handel 后发送的 response 丢失了,这时 1 无法 commit 新的 entry, 也无法发送同样的快照给2, 集群就这样永远不可用了. l.Index2Position(i) &lt; len(l.entries)","categories":[{"name":"database project","slug":"database-project","permalink":"https://moonm3n.github.io/categories/database-project/"}],"tags":[{"name":"TinyKV","slug":"TinyKV","permalink":"https://moonm3n.github.io/tags/TinyKV/"}]},{"title":"TinyKV Project2 RaftKV PartC","slug":"tinykv-project2-RaftKV-PartC","date":"2022-02-24T01:48:10.000Z","updated":"2022-04-19T02:36:59.288Z","comments":true,"path":"2022/02/24/tinykv-project2-RaftKV-PartC/","link":"","permalink":"https://moonm3n.github.io/2022/02/24/tinykv-project2-RaftKV-PartC/","excerpt":"TinyKV Project2 Part C, 实现快照机制, 定期对日志进行压缩.","text":"TinyKV Project2 Part C, 实现快照机制, 定期对日志进行压缩. snapshot 的实现分为两个部分. 实现日志的定期清理. 实现 snapshot 数据的发送. 问题记录1. panic: requested entry at index is unavailable这个错误发生在节点重启, 从 storage 中恢复 entries 时. debug 发现 storage 中仅存储了 lo 所在的那一个 entry, 没有 lo 到 hi 之间的 entries.回归测试 2b 后发现也出现了这个问题, 应该是写 2c 时影响到了. 修改 SaveRaftReady() 中应用 snapshot 和 append entries 的顺序后修复. 2. FAIL: TestSnapshotUnreliableRecoverConcurrentPartition2C没有任何错误提示.看日志发现 leader 在不停地 send append.debug 发现 first index 小于 truncated index 了, 怀疑 truncated index 或者 first index 的持久化有问题.debug 发现 send snapshot 的逻辑有问题, 无法发送 snapshot. 3. FAIL: panic: runtime error: slice bounds out of range [1186:384]更改 raft log 的 first index 时没有修改 entries 数组, 导致 从 entries 数组中取 entry 的逻辑出错. 4. panic: Key not foundleader 请求生成 snapshot 后分区恢复, 出现了新 leader, 新 leader 再次请求 snapshot 时便会触发这个问题.apply snapshot 时 WriteRegionState 后解决. 5. panic: request timeoutleader 一直在请求 snapshot, 随后就 timeout 了.apply snapshot 时 WriteRegionState 后解决.","categories":[{"name":"database project","slug":"database-project","permalink":"https://moonm3n.github.io/categories/database-project/"}],"tags":[{"name":"TinyKV","slug":"TinyKV","permalink":"https://moonm3n.github.io/tags/TinyKV/"}]},{"title":"Tinykv Project2 RaftKV PartB","slug":"tinykv-project2-RaftKV-PartB","date":"2022-02-03T09:56:59.000Z","updated":"2022-04-18T12:08:19.446Z","comments":true,"path":"2022/02/03/tinykv-project2-RaftKV-PartB/","link":"","permalink":"https://moonm3n.github.io/2022/02/03/tinykv-project2-RaftKV-PartB/","excerpt":"TinyKV Project2 Part B, 利用 Raft 模块构建容错的 KV 存储服务.","text":"TinyKV Project2 Part B, 利用 Raft 模块构建容错的 KV 存储服务. Basic在 Project2 Part A 中我们实现了 Raft 算法的 Leader election 和 Log replication 部分. 也实现了 Ready 相关的几个函数, 能够抽取 Raft 层的变更信息. 整个 2A 的主要工作都是在 Raft 层. 在 Project2 Part B 中, 我们把 KV 存储引擎用作 Raft 中的状态机, 将 Client 的请求作为日志, 扩展出一个容错的 KV 存储服务. 首先需要了解几个基本的概念: Store: 一个 tinykv-server. Peer: tinykv-server 中运行的一个 Raft node, 一个 Store 上可能运行有多个 Peer, 每个 Peer 所属于不同的 Region, 在 2B 中, 只会涉及到一个 Region. Region: Raft group. Implement peer storage在这一步中, 我们需要将 Raft 层的状态持久化. Raft 论文中提到的需要持久化的状态有: currentTerm: 节点当前的任期, 既 HardState 中的 Term. votedFor: 节点给谁投了票, 既 HardState 中的 Vote. 如果不持久化 Vote, 节点投票后重启会产生一个节点在一个 Term 中投出两张票的现象. log[]: 节点当前的日志. 与 Raft 论文不同, tinykv 中还需要将 Commit 和 Region 信息持久化. tinykv 中的绝大多数 request 都是幂等的. 具体到代码中, 我们需要实现 SaveReadyState() 与 Append() 两个函数, 2B 中并不涉及快照, ApplySnapshot() 函数暂时不需要实现. Implement Raft ready processHandleRaftReady get the ready from Raft module. persisting log entries. applying committed entries. sending Raft message to other peers. 问题记录1. panic: find no region for 30203030303030303030raft.go newRaft()优先使用 storage 中的 confState.Nodes 的值. 2. panic: runtime error: index out of range [18446744073709551611] with length 1raft.go sendAppend()index 处理有问题 3. 空指针snap 的 response 需要携带 Region, cd 的 Txn 需要赋值, 否则会出现空指针异常. 4. can’t call command header on leader nWaitRespWithTimeout 超时了.router.peerSender 管道满了不停地向 router.peerSender 中发送消息, 导致管道堵塞, 引发卡死.Ready() 函数里没有清空 msg. 5. panic: [region 1] 2 unexpected raft log index— FAIL: TestPersistPartition2B (27.40s)panic: [region 1] 2 unexpected raft log index: lastIndex 33539 &lt; appliedIndex 33810 [recovered] 看起来像是 lastIndex 持久化的逻辑有问题, 重启节点时监测到 lastIndex 比 appliedIndex 更小, 引发 panic.修改了持久化的逻辑后变成了偶现 bug, 1% 几率出现. 修复问题 6 后消失. 6. panic: runtime error: index out of range [1091] with length 1087偶现 bug, 3% 几率出现.bug 出现时, 这两个 index 的差值绝大多数时候为 5, length 为 1087 时, 最后一个元素应该是 [1086].看了日志, 在 partition 情况下才会发生. leader progress 里存储的 next 比自身的 lastIndex 大了更多.加日志后实锤, append entries 返回了大于 leader.lastIndex 的 index, leader 用这个 index 更新了 next,于是出现了数组越界. 错误原因:becomeFollower 时将 Vote 设置为了 None, partition 后有可能选出两个 Leader. 7. panic: len(resp.Responses) != 1偶现 bug, 1% 几率出现.panic: len(resp.Responses) != 1 goroutine 439 [running]:github.com/pingcap-incubator/tinykv/kv/test_raftstore.(*Cluster).MustPutCF(0xc00011f5c0, 0x47f01a0, 0x7, 0xc22a37f5d0, 0xa, 0x10, 0xc22a37f5e0, 0x9, 0x10) /Users/moon/GolandProjects/tinykv/kv/test_raftstore/cluster.go:308 +0x24dgithub.com/pingcap-incubator/tinykv/kv/test_raftstore.(*Cluster).MustPut(…) /Users/moon/GolandProjects/tinykv/kv/test_raftstore/cluster.go:298github.com/pingcap-incubator/tinykv/kv/test_raftstore.GenericTest.func1(0x1, 0xc000001e00) /Users/moon/GolandProjects/tinykv/kv/test_raftstore/test_test.go:211 +0x41fgithub.com/pingcap-incubator/tinykv/kv/test_raftstore.runClient(0xc000001e00, 0x1, 0xc21c12fb60, 0xc21e28acf0) /Users/moon/GolandProjects/tinykv/kv/test_raftstore/test_test.go:27 +0x7acreated by github.com/pingcap-incubator/tinykv/kv/test_raftstore.SpawnClientsAndWait /Users/moon/GolandProjects/tinykv/kv/test_raftstore/test_test.go:37 +0xb2FAIL github.com/pingcap-incubator/tinykv/kv/test_raftstore 23.630sFAILrm -rf /tmp/test-raftstore 修复问题 6 后消失.","categories":[{"name":"database project","slug":"database-project","permalink":"https://moonm3n.github.io/categories/database-project/"}],"tags":[{"name":"TinyKV","slug":"TinyKV","permalink":"https://moonm3n.github.io/tags/TinyKV/"}]},{"title":"TinyKV Project2 RaftKV PartA","slug":"tinykv-project2-RaftKV-PartA","date":"2022-01-17T07:30:04.000Z","updated":"2022-04-19T07:16:02.799Z","comments":true,"path":"2022/01/17/tinykv-project2-RaftKV-PartA/","link":"","permalink":"https://moonm3n.github.io/2022/01/17/tinykv-project2-RaftKV-PartA/","excerpt":"TinyKV Project2 Part A, 实现基本的 Raft 算法.","text":"TinyKV Project2 Part A, 实现基本的 Raft 算法. 目标实现基本的 Raft 算法, 包括 Leader election 和 Log replication. 2AA Leader electionRaft 算法中有三种状态, Leader, Candidate 和 Follower. Leader 负责处理所有的 Client 请求, Follower 只被动地响应 Leader 或 Candidate 的请求. Candidate 负责发起选举并处理选举结果.Raft 算法中的状态及状态间的转换关系如下图所示:在这一步, 我们要实现 Raft 层的 Leader election 部分. Leader election 流程Raft 算法使用随机计时器来选举 Leader, 集群中通常只有一个 Leader, Leader 会定期向所有其他节点广播心跳来彰显自己的领导权. 节点的状态并不会持久化, 所有节点都是以 Follower 状态启动或重启. Leader election 的整体流程为: 开始选举. 收到大多数确认选票, 成为 Leader. 收到大多数否定选票, 成为 Follower. 收到 Leader 消息, 成为 Follower. 超时重新开始选举. 更细节的问题: 什么时候开始选举? 总的来说, 当一个节点认为集群中没有 Leader 或者 Leader 已经宕机时, 它会转变为 Candidate 并发起选举. 只有 Candidate 才能发起选举. 对 Follower 而言, 在超时时间内没有收到来自 Leader 的心跳, 或者没有给 Candidate 投票时, 它会转变为 Candidate 并发起选举. 收到来自 Leader 的心跳说明集群中存在正在运行的 Leader, 投票给 Candidate 说明 Follower 认可该节点成为 Leader. 对 Candidate 而言, 在超时时间内没有收到足够的确认选票时, 它会自增 term 再次开始选举. 对 Leader 而言, 除非发现了集群中有 term 更高的 Leader 存在, 否则它不会放弃自己的领导权. Leader 不会转变为 Candidate, 也就不会发起选举. Candidate 怎样开始选举? 节点转变为 Candidate 后首先自增 term. 为自己投票. 重制选举超时计时器. 发送请求投票的 RPC 给其他所有服务器. Candidate 怎样结束选举? 接收到大多数节点的确认选票, 转变为 Leader 并立即向其他节点发送心跳. 发现了其他 Leader, 且这个 Leader 的 term 不小于 自己的 term. 选举超时, Candidate 会再次自增 term, 然后重新选举. 节点怎样投票? 每个节点每个 term 只会给一个节点投票, 再收到其他节点的 RequestVote 时, 直接拒绝. 申请选票的 Candidate 必须有更高的 term, 否则拒绝. 申请选票的 Candidate 必须有更新的日志, 否则拒绝. Leader election 实现根据文档的描述, 我们首先从 raft.Raft.tick() 开始. raft.Raft.tick() 的作用是递增逻辑时钟, 从而驱动选举超时或心跳超时. Raft 根据 term 来判断消息是否过期, 不需要比较消息发送时的逻辑时间, 因此, 不需要一个变量来记录实际的逻辑时间, 只需要用两个变量来处理选举超时和心跳超时即可.观察一下 Raft 结构体的成员, 在 TinyKV 中, 这两个变量是 heartbeatElapsed 和 electionElapsed.heartbeatElapsed 对 Leader 而言记录了上次心跳超时以来的 tick 数, 对其他状态无效.electionElapsed 对 Leader 和 Candidate 而言, 记录了上次选举超时以来的 tick 数; 对 Follower 而言, 记录了上次收到 Leader 的有效消息以来的 tick 数. raft.Raft.tick() 中只需要递增这两个值, 然后根据节点的状态处理超时即可. 需要注意的是, 选举超时的时间应该是随机的. 如果节点是 Leader 并发生了心跳超时 -&gt; 广播心跳. 如果节点是 Follower 并发生了选举超时 -&gt; 发起选举. 如果节点是 Candidate 并发生了选举超时 -&gt; 重新发起选举. 然后实现状态转换的几种方法, becomeFollower, becomeCandidate 和 becomeLeader. 接下来需要在 raft.Raft.Step() 中处理 MessageType_MsgRequestVote, MessageType_MsgRequestVoteResponse, MessageType_MsgHup 和 MessageType_MsgBeat 这四种类型的消息. MessageType_MsgRequestVote 2AA 并不会涉及到 Raft 的日志, 只要根据 term 大小以及自己是否在该任期投票, 返回拒绝或接受即可. 当消息的 term 比自己的更大时, 需要先转变为 Follower, 将自己的 term 设置为消息的 term, Lead 和 Vote 设置为 None, 再判断应该拒绝还是接受. MessageType_MsgRequestVoteResponse 首先将结果添加到记录选票的 raft.Raft.votes 中, 然后统计收到的选票. 如果收到的肯定票更多, 成为 Leader; 如果收到的否定票更多, 成为 Follower; 如果都不是, 直接返回, 等待其他节点的选票. MessageType_MsgHup 测试时发现的消息类型, 收到时节点应该先转变为 Candidate, 然后发起选举. MessageType_MsgBeat 这种消息也是测试时发现的, 收到消息时直接广播心跳. MessageType_MsgHup, MessageType_MsgBeat, 以及 2AB 中会遇到的 MessageType_MsgPropose, 3A 中会遇到的 MessageType_MsgTransferLeader 和 MessageType_MsgTimeoutNow 都是 local message. local message 可以理解为自己发送给自己的消息. 它们都是不带任期的, 即 term 为 0. 然后需要考虑怎样发送消息. 通过文档我们知道, 在 2A 中我们不需要实际地发送消息, 只需要将它们添加到 raft.Raft.msgs 中即可. 在 TinyKV 中, Raft 层不会直接交互, 而是等待上层来读取并发送消息. 最后实现 raft.Raft.NewRaft(), 这里没什么好说的. 结构体中不认识的成员设置成默认值, 然后面向测试编程. 2AB Log replicationRaft 的日志是一个 entry 数组. 每个 entry 由 index 和 term 唯一标识. 论文中的这两句话, 描述了日志匹配的特性: 如果在不同的日志中的两个条目拥有相同的索引和任期号, 那么他们存储了相同的指令. 如果在不同的日志中的两个条目拥有相同的索引和任期号, 那么他们之前的所有日志条目也全部相同. Log replication 是 Raft 达成共识的关键. 在这一步, 我们要实现 Raft 层的 Log replication 部分. Log replication 相关概念日志条目的状态流转日志的结构为: snapshot/first…..applied….committed….stabled…..last last: 最后一个条目的 index. stabled: 已经持久化的条目的最大 index. committed: 已经提交的条目的最大 index. applied: 已经应用的条目的最大 index. first: 第一个条目的 index. 添加日志时, 先将条目存放在内存中, 此时条目位于 (stable, last] 中,然后由 Peer 层将条目持久化, 此时条目位于 (committed, stabled] 中,再由 Raft 层将条目提交, 表示条目可以被应用, 此时条目位于 (applied, committed] 中,最后由 Peer 层将条目应用, 此时条目位于 [first, applied] 中. Leader 为其他节点维护的变量Leader 通过 nextIndex 和 matchIndex 来保存其他节点的状态. matchIndex 为已知的已经复制到该服务器的最高日志条目的索引.nextIndex 发送到该节点的下一个日志条目的索引. 每个节点成为 Leader 时, 会将所有其他节点的 nextIndex 初始化为 Leader 的 lastIndex + 1, matchIndex 初始化为 0. Log replication 流程 成为 Leader 时, 初始化所有其他节点的 nextIndex 和 matchIndex. 向自己的日志中添加一个新条目. 广播日志. 接收到 Client 请求时, 将请求作为新条目添加到自己的日志中, 然后广播日志. 收到心跳 response 时, 如果节点的 matchIndex 小于自己的 lastIndex, 广播日志. 收到 Append response 时, 如果添加失败, 修改对应节点的 nextIndex 并重试; 如果添加成功, 修改对应节点的 matchIndex 与 nextIndex, 然后计算大多数节点的 matchIndex, 如果比 commit 更大, 推进 commit. 节点收到 Append 消息时, 通过检查消息的 index 与 term 是否与自己的匹配来决定是否接受这些条目. 更细节的问题: 节点如何检查消息的 index 与 term? 首先判断消息的 index 是否比自己的 lastIndex 更大, 如果是, 说明自己的日志太少了, 无法 Append 这些消息, 在 Raft 中, 日志条目的添加应该是连续的. 再判断消息的 term 是否与自己的索引为 index 的条目的 term 一致, 如果不是, 说明自己与 Leader 的日志在 [first, index] 存在不一致. Leader 应该发送更早的日志, 让 Follower 覆盖这些不一致. 最后将消息中的所有条目添加或覆盖到自己的 entries 中. Log replication 实现首先要确定日志条目的存储位置, Raft 结构体中有 RaftLog 这样一个成员变量, 它的主要作用是管理 Raft log. RaftLog 中也有一个 Storage, 但这个 Storage 与 Project1 中的 Storage 不同, Project1 中的 Storage 是整个 TinyKV 存储层的抽象, 这里的 Storage 是 Raft 数据存储的抽象. 在 Raft 论文中, 所有日志条目都是持久化的, 在 TinyKV 的实现中, 条目先存放在 raft.RaftLog.entries 也就是内存中, 再由上层持久化到 raft.Storage 中, 同时内存中始终保持条目的副本. 这样做应该能够提高读写效率. Raft 节点启动时会尝试从 Storage 中获取日志以及节点的状态, 这也是为什么 raft.newLog 需要传入一个 Storage 参数. 在 2AB 中, 我们需要在 raft.Raft.Step() 中处理一些与日志相关的消息. MessageType_MsgPropose 这也是一种 local message, 表示向 Leader 的日志中添加条目, 只有 Leader 会处理这一消息. 在这里我们首先需要给这些条目设置 index 以及 term. index 从 lastIndex 递增即可, term 为 Leader 的 term. 接着更新关于自己的 matchIndex, 用于推进 commit. 最后广播 Append. MessageType_MsgAppend 在这里按照之前描述的方式, 检查消息的 index 与 term, 然后添加或覆盖即可. 如果消息携带的 commit 比自己的 commit 更大, 推进 commit. MessageType_MsgAppendResponse Append 成功时, 尝试推进 commit; Append 失败时, 重新尝试发送更早的日志. MessageType_MsgHeartbeat 首先根据 term 自己是否需要转变为 Follower, 重制自己的选举超时. 如果消息携带的 commit 比自己的 commit 更大, 推进 commit. 最后返回 response. MessageType_MsgHeartbeatResponse 检查 Follower 的 matchIndex 是否小于自己的 lastIndex, 如果是, 发送 Append. 需要注意的是: 如果集群中只有 Leader 一个节点, 当它把条目添加到自己的日志中时, 这个条目就应该被 commit. Leader 不会向自己发送 Append 消息, 也就不会收到来自自己的 Append response, 无法在处理 response 时推进 commit. TinyKV 将心跳拆成了 Append 与 Heartbeat 两种消息, 每个消息的职责更加清晰. Append 消息主要用于同步日志, Heartbeat 消息主要用于彰显领导权. Append 消息与 Heartbeat 消息中 commit 的取值是不同的. Follower 接收 Append 消息时会检查 index 与 term, 成功 Append 时, 能够保证自己的日志与 Leader 的日志没有冲突. 所以 Append 消息直接使用 Leader 的 commit 即可. Follower 接收 Heartbeat 消息时, 并不会进行上述检查. 如果 Heartbeat 消息直接使用 Leader 的 commit 来推进自己的 commit, 可能会 commit 冲突的条目. 所以 Heartbeat 消息的 commit 需要使用 min(r.RaftLog.committed, r.Prs[to].Match) 2AC Raw node interface在 TinyKV 的架构中, Raft 层并不负责持久化状态, 节点间也不会直接交互, 而是由上层应用定期获取 Raft 层的状态变更以及节点想要发送的消息, 由上层应用来持久化状态和发送消息. RawNode 便是 Raft 层与上层应用的桥梁. 上层应用通过 raft.RawNode.HasReady() 来判断是否有变更, 通过 raft.RawNode.Ready() 来获取 Raft 层的变更, 再通过 raft.RawNode.Advance() 将上层对 Raft 层的变更应用到 Raft 层. 这里实现的几个方法相对比较简单.","categories":[{"name":"database project","slug":"database-project","permalink":"https://moonm3n.github.io/categories/database-project/"}],"tags":[{"name":"TinyKV","slug":"TinyKV","permalink":"https://moonm3n.github.io/tags/TinyKV/"}]},{"title":"Raft-论文研读","slug":"Raft-论文研读","date":"2022-01-17T05:59:05.000Z","updated":"2022-01-17T16:00:51.764Z","comments":true,"path":"2022/01/17/Raft-论文研读/","link":"","permalink":"https://moonm3n.github.io/2022/01/17/Raft-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/","excerpt":"","text":"","categories":[{"name":"paper","slug":"paper","permalink":"https://moonm3n.github.io/categories/paper/"}],"tags":[{"name":"Raft","slug":"Raft","permalink":"https://moonm3n.github.io/tags/Raft/"}]},{"title":"TinyKV Project1 StandaloneKV","slug":"tinykv-project1-StandaloneKV","date":"2022-01-16T11:37:20.000Z","updated":"2022-04-18T04:38:03.090Z","comments":true,"path":"2022/01/16/tinykv-project1-StandaloneKV/","link":"","permalink":"https://moonm3n.github.io/2022/01/16/tinykv-project1-StandaloneKV/","excerpt":"TinyKV Project1, 基于 badger 构造一个单机的支持列族存储的 gRPC 服务.","text":"TinyKV Project1, 基于 badger 构造一个单机的支持列族存储的 gRPC 服务. 目标基于 badger 构造一个单机的支持列族存储的 gRPC 服务.这一服务提供四种基本操作: Put/Delete/Get/Scan. Put: 向指定列族中写入. Get: 从指定列族中读取. Delete: 删除指定列族的指定值. Scan: 从指定列族中顺序读取多个值. Implement standalone storage engine题目解析在这一步中我们要实现 Storage 接口的一个实现类 StandAloneStorage, 在 TinyKV 中, Storage 接口有三个实现类: MemStorage, RaftStorage 和 StandAloneStorage. StorageStorage 可以理解为存储层的抽象, 提供 Start(), Stop(), Write(), Reader() 四种方法. Start() 与 Stop() 方法只有 RaftStorage 类会用到, 用于启动/停止底层 Raft 节点, 题目中也没有要求我们实现这两个方法. ( 感觉 Start() 与 Stop() 方法中的两行 // Your Code Here (1). 应该删掉 ) Write() 方法向存储层中写入变更, 变更的类型可以是 Put 或 Delete. Reader() 方法返回一个 StorageReader 类, 提供 GetCF(), IterCF(), Close() 三种方法.总的来说, Storage 提供 Write(), GetCF() 和 IterCF() 三种操作数据的方法. MemStorageMemStorage 是 Storage 的纯内存实现, 硬编码了三个列族: CfDefault, CfLock 和 CfWrite. 每个列族是一颗红黑树,Write(), GetCF(), IterCF() 直接调用了红黑树的 ReplaceOrInsert(item)、Delete(item)、Get(item) 方法. Project 4 测试时使用的 Storage 便是 MemStorage. RaftStorageRaftStorage 是 Storage 的分布式实现. 当我们 Run TinyKV with TinySQL 时, 默认会使用这个 Storage. StandAloneStorageStandAloneStorage 是 Storage 的单机实现, 我们要实现的便是这它. 实现思路看了题目之后一脸懵逼, 还好 Storage 接口还有其他两个实现类 MemStorage 与 RaftStorage. 读了读它们的代码, 题目中要求基于 badger key/value API, 而 MemStorage 使用的是红黑树, 只能参考一下 Write() 时处理 Modify 的方法, 其他部分没有参考价值.还剩下 RaftStorage , RaftStorage Start() 时根据 config 新建了一些 client 和 worker 并启动, Stop() 时将它们停止.Write() 时, RaftStorage 将 Modify 转换为 Put 或 Delete, 然后将它们打包成 request 发送到 Raft 层. 这里并没有写入 badger 相关的代码.GetCF() 时, RaftStorage 直接调用 engine_util 中的方法在 badger 中读取, IterCF() 也类似.看了看 engine_util 中的方法, 原来题目中提到的 badger 的操作都在这, 接下来怎么写就比较清晰了. StandAloneStorage 的 Write() 方法只要将 Modify 解析为 Put 或 Delete, 再调用 engine_util 中的方法写入或删除就可以了.Reader() 方法只要新建一个事务, 再新建一个类似 RegionReader 的 Reader 即可.Reader Close() 时记得调用 Discard() 方法结束事务. Implement service handlers题目解析在上一步我们实现了 StandAloneStorage, 但它的接口和题目中咱们的最终目标 Put/Delete/Get/Scan 还不太一样,在这一步中我们要利用 StandAloneStorage 实现 RawPut/RawDelete/RawGet/RawScan 这四个方法, 处理 request, 返回对应的 response. 实现思路要实现的四个方法属于 Server 这个类, Server 类中正好有 Storage 这一属性, 这算是衔接上了.Region 是 Multi-Raft 中的一个概念, 在这一步中我们还没有涉及到 Raft, 不知道为什么类似 RawGetResponse 这样的类中会有 RegionError 这样的属性, 还是先忽略吧. RawGet从 Storage 中获取 Reader, 再调用 Reader 的 GetCF 即可. 注意出错时要将错误信息赋值给 response.Error, 没找到结果时要将 response.NotFound 设置为 True. RawPut &amp; RawDelete先构建 Modify, 再调用 Storage 的 Write 方法即可. RawScan从 Storage 中获取 Reader, 再从 Reader 中获取 Iter, Seek 到对应的 Key 读取最多 Limit 个值即可. 其他在 macOS 上切换 go 版本用 1.17.5 版本的 go 运行 project 时会有奇怪的 errorfatal error: unexpected signal during runtime execution.看了一下 issues 应该是 go 版本的锅, 降级到 1.16.x 便可. 在 macOS 中使用 brew 可以方便地管理软件的版本. 1234567# 安装 gobrew install go# 安装 go 1.16brew install go@1.16# 切换到 go 1.16brew unlink gobrew link go@1.16","categories":[{"name":"database project","slug":"database-project","permalink":"https://moonm3n.github.io/categories/database-project/"}],"tags":[{"name":"TinyKV","slug":"TinyKV","permalink":"https://moonm3n.github.io/tags/TinyKV/"}]},{"title":"oceanbase-competition-final","slug":"oceanbase-competition-final","date":"2021-12-25T14:07:57.000Z","updated":"2022-01-06T13:18:59.119Z","comments":true,"path":"2021/12/25/oceanbase-competition-final/","link":"","permalink":"https://moonm3n.github.io/2021/12/25/oceanbase-competition-final/","excerpt":"","text":"doing","categories":[{"name":"paper","slug":"paper","permalink":"https://moonm3n.github.io/categories/paper/"}],"tags":[]},{"title":"15-445 BUFFER POOL","slug":"CMU-15-445-PROJECT-1-BUFFER-POOL","date":"2021-12-14T13:49:17.000Z","updated":"2021-12-14T14:10:22.955Z","comments":true,"path":"2021/12/14/CMU-15-445-PROJECT-1-BUFFER-POOL/","link":"","permalink":"https://moonm3n.github.io/2021/12/14/CMU-15-445-PROJECT-1-BUFFER-POOL/","excerpt":"","text":"","categories":[{"name":"database course","slug":"database-course","permalink":"https://moonm3n.github.io/categories/database-course/"}],"tags":[{"name":"cmu 15-445","slug":"cmu-15-445","permalink":"https://moonm3n.github.io/tags/cmu-15-445/"}]},{"title":"oceanBase 数据库大赛","slug":"oceanBase-数据库大赛","date":"2021-11-25T06:49:26.000Z","updated":"2022-01-06T07:15:16.780Z","comments":true,"path":"2021/11/25/oceanBase-数据库大赛/","link":"","permalink":"https://moonm3n.github.io/2021/11/25/oceanBase-%E6%95%B0%E6%8D%AE%E5%BA%93%E5%A4%A7%E8%B5%9B/","excerpt":"","text":"赛题描述在开源版本 OceanBase 的基础上, 针对 Nested Loop Join 场景做性能优化. 采用 sysbench 基准测试中 Throughput 的 events/s (eps) 这一项作为排名依据. 赛题解析什么是 Nested Loop Join?Nested Loop Join 是一种常见的数据库查询操作, 其中两个表的数据量相对较小, 且两个表的关联关系相对较简单.Nested Loop Join 的基本原理是每次从左表获取一行, 然后用这行数据和右表进行 Join. 与右表进行 Join 时, 可以通过索引查询降低复杂度. 表结构123456789101112131415local queryquery = string.format([[ CREATE TABLE t%d( c1 int primary key, c2 int, c3 int, v1 CHAR(60), v2 CHAR(60), v3 CHAR(60), v4 CHAR(60), v5 CHAR(60), v6 CHAR(60), v7 CHAR(60), v8 CHAR(60), v9 CHAR(60) )]], table_id)do_query(drv, con, &quot;create index t2_i1 on t2(c2) local&quot;)do_query(drv, con, &quot;create index t2_i2 on t2(c3) local&quot;)ival = sysbench.rand.default(1, sysbench.opt.table_size)left_min = ival - 100;left_max = ival + 100;cond = string.format(&quot;A.c1 &gt;= %d and A.c1 &lt; %d and A.c2 = B.c2 and A.c3 = B.c3&quot;, left_min, left_max)query = &quot;select /*+ordered use_nl(A,B)*/ * from t1 A, t2 B where &quot; .. cond 查询语句123456789101112131415161. 原始查询语句 select /*+ordered use_nl(A,B)*/ * from t1 A, t2 B where A.c1 &gt;= 100 and A.c1 &lt; 200 and A.c2 = B.c2 and A.c3 = B.c3; select /*+ordered use_nl(A,B)*/ * from t1 A, t2 B where A.c1 &gt;= 100 and A.c1 &lt; 200 and A.c2 = B.c2 and A.c3 = B.c3; explain select /*+ordered use_nl(A,B)*/ * from t1 A, t2 B where A.c1 &gt;= 100 and A.c1 &lt; 200 and A.c2 = B.c2 and A.c3 = B.c3;2. 当 A.c1 = A.c2 时, 改写后的查询语句 select /*+ordered use_nl(A,B)*/ * from t1 A, t2 B where A.c1 &gt;= 100 and A.c1 &lt; 200 and B.c2 &gt;= 100 and B.c2 &lt; 200 and A.c3 = B.c3; select /*+ordered use_nl(A,B)*/ * from t1 A, t2 B where A.c1 &gt;= 100 and A.c1 &lt; 200 and B.c2 &gt;= 100 and B.c2 &lt; 200 and A.c3 = B.c3; explain select /*+ordered use_nl(A,B)*/ * from t1 A, t2 B where A.c1 &gt;= 100 and A.c1 &lt; 200 and B.c2 &gt;= 100 and B.c2 &lt; 200 and A.c3 = B.c3;","categories":[{"name":"paper","slug":"paper","permalink":"https://moonm3n.github.io/categories/paper/"}],"tags":[]},{"title":"GFS 论文研读","slug":"GFS-论文研读","date":"2021-11-13T05:27:52.000Z","updated":"2022-01-17T15:59:25.181Z","comments":true,"path":"2021/11/13/GFS-论文研读/","link":"","permalink":"https://moonm3n.github.io/2021/11/13/GFS-%E8%AE%BA%E6%96%87%E7%A0%94%E8%AF%BB/","excerpt":"","text":"","categories":[{"name":"paper","slug":"paper","permalink":"https://moonm3n.github.io/categories/paper/"}],"tags":[]}],"categories":[{"name":"databases","slug":"databases","permalink":"https://moonm3n.github.io/categories/databases/"},{"name":"database project","slug":"database-project","permalink":"https://moonm3n.github.io/categories/database-project/"},{"name":"paper","slug":"paper","permalink":"https://moonm3n.github.io/categories/paper/"},{"name":"database course","slug":"database-course","permalink":"https://moonm3n.github.io/categories/database-course/"}],"tags":[{"name":"TiDB","slug":"TiDB","permalink":"https://moonm3n.github.io/tags/TiDB/"},{"name":"TinyKV","slug":"TinyKV","permalink":"https://moonm3n.github.io/tags/TinyKV/"},{"name":"Raft","slug":"Raft","permalink":"https://moonm3n.github.io/tags/Raft/"},{"name":"cmu 15-445","slug":"cmu-15-445","permalink":"https://moonm3n.github.io/tags/cmu-15-445/"}]}